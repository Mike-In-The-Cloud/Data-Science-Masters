{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP1801 - Machine Learning \n",
    "## University of Greenwich\n",
    "## 001002629\n",
    "\n",
    "###### Classification Theory on cifar10 dataset\n",
    "##### Tensorflow 2.2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #  provides a multidimensional array object\n",
    "import matplotlib as plt # library for visualizations, graphs\n",
    "import tensorflow as tf # machine learning framework\n",
    "import matplotlib.pyplot as plt # graph plotting library\n",
    "from sklearn.metrics import confusion_matrix # plotting tool for confususion matrix\n",
    "import itertools # Functions creating iterators for efficient looping\n",
    "import random # used to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules from tensorflow.keras to keep code readable\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorflow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "PATH = \"./\" # parent folder to save all data\n",
    "MODEL_NAME = \"005_6Conv_3Dense_Adam_DG\" # name of model\n",
    "\n",
    "# used for the dataset to change the numerical system\n",
    "# to a named labelling system\n",
    "labels = '''airplane\n",
    "automobile\n",
    "bird\n",
    "cat\n",
    "deer\n",
    "dog\n",
    "frog\n",
    "horse\n",
    "ship\n",
    "truck'''.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DATA_GENERATOR = False # bool used to switch between using data generator during training\n",
    "BATCH_SIZE = 16 # batch size to pass while training\n",
    "EPOCS = 50 # number of epochs to run\n",
    "ACTIVATION_LAYER = \"relu\"  # activation function\n",
    "ACTIVATION_OUTPUT = \"softmax\" # output activation function\n",
    "OPTIMIZER = Adam(lr=0.0001) # optimiser\n",
    "LOSS = \"categorical_crossentropy\" # loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating directories for images and models to be saved\n",
    "import os\n",
    "\n",
    "# Model data files\n",
    "if not os.path.exists('%s/Model_Data/%s' %(PATH, MODEL_NAME)):\n",
    "    os.makedirs('%s/Model_Data/%s' %(PATH, MODEL_NAME))\n",
    "    print(\"***\\nNew model directory created\\n***\")\n",
    "else:\n",
    "    print(\"***\\nModel directory already present\\n***\")\n",
    "    \n",
    "# dataset files\n",
    "if not os.path.exists('%s/dataset'%(PATH)):\n",
    "    os.makedirs('%s/dataset'%(PATH))\n",
    "    print(\"***\\nNew dataset directory created\\n***\")\n",
    "else:\n",
    "    print(\"***\\nDataset directory already present\\n***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that are used throughout the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes numpy array and checks unique element count, returns array\n",
    "def count_freq(numpy_data):\n",
    "    frequencies = np.sum(numpy_data, axis=0)\n",
    "    \n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print out values for dataset\n",
    "def array_values(image_array, labal_array, data_set):\n",
    "    print(\"%s Image values:\" % data_set)\n",
    "    print('Image values (min to max):', np.min(image_array), 'to', np.max(image_array))\n",
    "    print('Label values (min to max):', np.min(labal_array), 'to', np.max(labal_array))\n",
    "    if(data_set == \"Training\"):\n",
    "        data = \"train\"\n",
    "    else:\n",
    "        data = \"test\"\n",
    "    print(\"X_{0} shape: {1}\".format(data, image_array.shape))\n",
    "    print(\"y_{0} shape: {1}\".format(data, labal_array.shape))\n",
    "    print('Total samples per class:', np.sum(labal_array, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to normalize arrays to values between 0 and 1\n",
    "def normalise_data(array):\n",
    "    output = array.astype(\"float32\") / 255\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for one hot encoding\n",
    "def one_hot(array):\n",
    "    output = tf.keras.utils.to_categorical(array)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes numpy array and checks unique element count, returns array\n",
    "def count_freq(numpy_data):\n",
    "    (unique, counts) = np.unique(numpy_data, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T   # hold the frequency of each value\n",
    "    return frequencies    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes in numpy array and string, output bar graphs showing \n",
    "# distribution of training and testing data\n",
    "def plot_freq(frequency, title):\n",
    "    #labels = frequency[:,0] # labels for x axis\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1]) # axes is absolute coordinates on plot\n",
    "    \n",
    "    ax.bar(frequency[0:10, 0],frequency[0:10, 1]) # x and y axis for plot\n",
    "    plt.xticks(frequency[0:10, 0], labels, rotation = 45) # define the x labels\n",
    "    plt.title(title) # title\n",
    "    plt.xlabel(\"Class\") # x axis label\n",
    "    plt.ylabel(\"Number of images\")# y axis label\n",
    "    # loop to get frequency data and display on chart\n",
    "    for index,data in enumerate(frequency[0:10, 1]):\n",
    "        # position the text data \n",
    "        plt.text(x=index , y =data+1 , s=f\"{data}\" , fontdict=dict(fontsize=10),ha=\"center\")\n",
    "    if not os.path.exists(\"%s/Images/%s.png\"% (PATH,title)):\n",
    "        plt.savefig(\"%s/Images/%s.png\"% (PATH,title),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history graphs from model training\n",
    "def plot_history_graphs(data1, data2):\n",
    "    plt.plot(history.history[data1], label = data1)\n",
    "    plt.plot(history.history[data2], label = data2)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.title('Model %s'%data1)\n",
    "    plt.ylabel(data1)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    if not os.path.exists(\"%s/Model_Data/%s/%s.png\"% (PATH,MODEL_NAME,data1)):\n",
    "        plt.savefig(\"%s/Model_Data/%s/%s.png\"% (PATH,MODEL_NAME,data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot random images from the dataset\n",
    "def image_plot(Image_array, Label_array, Cols, Classes = 10, title = \"Data Visulisation\"):\n",
    "    image_array = Image_array # image array\n",
    "    label_array = Label_array # label array\n",
    "    cols = Cols # number of columns for visulisation\n",
    "    num_classes = Classes # classes \n",
    "    num_of_samples = [] # empty list\n",
    " \n",
    "    # figure formatting\n",
    "    fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(cols,num_classes))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # nested for loops to generate multiple images from data set\n",
    "    for i in range(cols):\n",
    "        for j in range(num_classes):\n",
    "          x_selected = image_array[label_array == j]\n",
    "          axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))\n",
    "          axs[j][i].axis(\"off\")\n",
    "          if i == 2:\n",
    "            axs[j][i].set_title(str(j))\n",
    "            num_of_samples.append(x_selected)\n",
    "    if not os.path.exists(\"%s/Images/%s.png\"% (PATH,title)):\n",
    "        plt.savefig(\"%s/Images/%s.png\"% (PATH,title),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix to see what is going on with \n",
    "# model prediction\n",
    "def confusion_matrix_graph(cm, classes,\n",
    "                    title = \"Confusion Matrix\",\n",
    "                    cmap = plt.cm.Blues):\n",
    "    \n",
    "    #cm = cm.astype(\"float\") / cm.sum(axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i,j], \"d\"),\n",
    "        horizontalalignment = \"center\",\n",
    "        color = \"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    if not os.path.exists(\"%s/Model_Data/%s/%s.png\"% (PATH,MODEL_NAME,title)):\n",
    "        plt.savefig(\"%s/Model_Data/%s/%s.png\"% (PATH,MODEL_NAME,title))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "This function is used to check if a pkl file of the dataset has been previously \n",
    "downloaded. \n",
    "If there is an existing file, use this file as the dataset\n",
    "else, use the TF api to download the dataset and save the dataset to local disk\n",
    "\"\"\"\n",
    "\n",
    "def load_cifar10():\n",
    "    # check to see if the dataset has been saved as a pickle file\n",
    "    if(os.path.isfile(\"%s/dataset/cifar10_data.pkl\"%(PATH))):\n",
    "        print(\"cifar10 data present.\")\n",
    "        with open(\"%s/dataset/cifar10_data.pkl\"%(PATH),'rb') as f:\n",
    "            cifar10 = pickle.load(f)\n",
    "        print('cifar10 loaded from local file.')\n",
    "    else:\n",
    "        print(\"Data not present\\nLoading from API\\n\")\n",
    "        # load in data from keras datasets\n",
    "        cifar10_data = tf.keras.datasets.cifar10\n",
    "        # assign data to variables\n",
    "        (X_train_raw, y_train_raw), (x_test_raw, y_test_raw) = cifar10_data.load_data()\n",
    "        cifar10 = {\n",
    "            \"train_images\": X_train_raw,\n",
    "            \"train_labels\": y_train_raw,\n",
    "            \"test_images\": x_test_raw,\n",
    "            \"test_labels\": y_test_raw,\n",
    "        }\n",
    "        with open(\"%s/dataset/cifar10_data.pkl\"%(PATH), \"wb\") as f:\n",
    "            pickle.dump(cifar10, f)\n",
    "        print(\"Data saved to local file\")\n",
    "    return cifar10\n",
    "\n",
    "\n",
    "cifar10_data = load_cifar10()\n",
    "\n",
    "print('\\ncifar10 Train images (x): shape=', cifar10_data['train_images'].shape)\n",
    "print(\"cifar10 Train image min/max values:\",np.min(cifar10_data['train_images']), 'to',np.max(cifar10_data['train_images']))\n",
    "print('cifar10 Train labels (y): shape=', cifar10_data['train_labels'].shape)\n",
    "print(\"cifar10 Train label min/max values:\",np.min(cifar10_data['train_labels']), 'to',np.max(cifar10_data['train_labels']))\n",
    "\n",
    "\n",
    "print('\\ncifar10 Test images (x): shape=', cifar10_data['test_images'].shape)\n",
    "print(\"cifar10 Test image min/max values:\",np.min(cifar10_data['test_images']), 'to',np.max(cifar10_data['test_images']))\n",
    "print('cifar10 Test labels (y): shape=', cifar10_data['test_labels'].shape)\n",
    "print(\"cifar10 Test label min/max values:\",np.min(cifar10_data['test_labels']), 'to',np.max(cifar10_data['test_labels']))\n",
    "\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training image values before normalisation (min to max):', np.min(cifar10_data['train_images']), 'to', np.max(cifar10_data['train_images']))\n",
    "print('Testing image values before normalisation (min to max):', np.min(cifar10_data['test_images']), 'to', np.max(cifar10_data['test_images']))\n",
    "# nomalise image data\n",
    "X_train = normalise_data(cifar10_data['train_images'])\n",
    "x_test = normalise_data(cifar10_data['test_images'])\n",
    "# check min/max\n",
    "print('Training image values after normalisation (min to max):', np.min(X_train), 'to', np.max(X_train))\n",
    "print('Testing image values after normalisation (min to max):', np.min(x_test), 'to', np.max(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "y_train = one_hot(cifar10_data['train_labels'])\n",
    "y_test = one_hot(cifar10_data['test_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values of the training reshaped/nomalised arrays\n",
    "array_values(X_train, y_train, \"Training\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values for the testing reshaped/normalised arrays\n",
    "array_values(x_test, y_test, \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data frequency\n",
    "ax = plot_freq(count_freq(y_test.argmax(axis = 1)), \"Distribution of testing dataset\")    \n",
    "\n",
    "# training data frequency\n",
    "ax2 = plot_freq(count_freq(y_train.argmax(axis = 1)), \"Distribution of training dataset\")    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the dataset\n",
    "image_plot(X_train, np.argmax(y_train, axis= 1), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tensorflow model using API method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model using tensorflow functional API\n",
    "# first filter size = 3 * 3 * 3 * 32\n",
    "first_layer = Input(shape = X_train[0].shape)\n",
    "\n",
    "x = Conv2D(32, (3,3),  activation = ACTIVATION_LAYER, padding = \"same\")(first_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(32, (3,3), activation = ACTIVATION_LAYER, padding = \"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(64, (3,3), activation = ACTIVATION_LAYER, padding = \"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3,3), activation = ACTIVATION_LAYER, padding = \"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Conv2D(128, (3,3), activation = ACTIVATION_LAYER, padding = \"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3,3), activation = ACTIVATION_LAYER, padding = \"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation = ACTIVATION_LAYER)(x)\n",
    "x = Dropout(0.6)(x)\n",
    "\n",
    "x = Dense(10, activation=ACTIVATION_OUTPUT)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(first_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = OPTIMIZER,\n",
    "              loss = LOSS,\n",
    "              metrics = (\"accuracy\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "if not os.path.exists('%s/Model_Data/%s/modelsummary.txt' %(PATH, MODEL_NAME)):\n",
    "    with open('%s/Model_Data/%s/modelsummary.txt' %(PATH, MODEL_NAME), 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "            print(\"Summary Written.\")\n",
    "else:\n",
    "    print(\"Summary already saved.\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess data on the fly using keras ImageDataGenerator\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range = 90, # Int. Degree range for random rotations.\n",
    "    width_shift_range = 0.1, #float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "    height_shift_range = 0.1,\n",
    "    horizontal_flip = True #Boolean. Randomly flip inputs horizontally.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of steps while generating data to iterate over entire dataset\n",
    "# using floor division\n",
    "STEPS_PER_EPOC = (X_train.shape[0] // BATCH_SIZE)\n",
    "# call to data generator\n",
    "train_generator = data_generator.flow(X_train,y_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "if(USE_DATA_GENERATOR == True):\n",
    "    history = model.fit(train_generator, \n",
    "                                  steps_per_epoch = STEPS_PER_EPOC, \n",
    "                                  epochs = EPOCS,\n",
    "                                  validation_data =(x_test, y_test),\n",
    "                                   shuffle=True)\n",
    "else:\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data =(x_test, y_test),\n",
    "                        epochs = EPOCS,\n",
    "                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss = plot_history_graphs(\"loss\", \"val_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = plot_history_graphs(\"accuracy\", \"val_accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "score_output = print('Test loss: %s \\nTest accuracy: %s' % (score[0], score[1]))\n",
    "\n",
    "if not os.path.exists('%s/Model_Data/%s/evaluation_score.txt' %(PATH, MODEL_NAME)):\n",
    "    with open('%s/Model_Data/%s/evaluation_score.txt' %(PATH, MODEL_NAME), 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print('Test loss: %s \\nTest accuracy: %s' % (score[0], score[1]))\n",
    "    print(\"Score Written.\")\n",
    "else:\n",
    "    print(\"Score Already present\")\n",
    "score_output\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preditctions = model.predict(x_test).argmax(axis = 1)\n",
    "cm = confusion_matrix(y_test.argmax(axis = 1), preditctions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix\n",
    "confusion_matrix_graph(cm, list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display misclassified examples\n",
    "misclassified = np.where(preditctions != y_test.argmax(axis = 1))[0]\n",
    "image = np.random.choice(misclassified)\n",
    "plt.imshow(x_test[image], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.title(\"True label: %s | Predicted: %s\" % (labels[y_test.argmax(axis = 1)[image]], labels[preditctions[image]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "if not os.path.exists(\"%s/Model_Data/%s/%s.h5\" % (PATH,MODEL_NAME,MODEL_NAME)):\n",
    "        model.save_weights(\"%s/Model_Data/%s/%s.h5\" % (PATH,MODEL_NAME,MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
